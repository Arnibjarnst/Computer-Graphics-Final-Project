<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

**Report**

Student Names: Árni Bjarnsteinsson and Gianluca Figini

# Implemented features

## Árni Bjarnsteinsson

### Images as Textures

**Relevant files:**
<ul>
    <li> <code>image_texture.cpp</code></li>
</ul>

<div class="twentytwenty-container">
    <img src="../../validation/imagesAsTextures/mitsuba_texture.png" alt="Mitsuba" class="img-responsive">
    <img src="../../validation/imagesAsTextures/nori_texture.png" alt="Nori" class="img-responsive">
</div>

### Simple Emitters (Spotlight)

**Relevant files:**
<ul>
    <li> <code>spotlight.cpp</code></li>
    <li> <code>spotlight_area.cpp</code></li>
</ul>

**Implementation:**

The implementation is very straight forward and is based on the pointlight, except now we also specify a maximum angle of illumnation and another (smaller) angle,
called the falloff angle. The light contribution is exactly like the pointlight if we are within the falloff angle else it is linearly interpolated from 1 to 0
till we reach the maximum angle.


**Validation:**

To validate our spotlight I created a scene in mitsuba and nori (note that the light intensity is slightly different but that is simply implementation detail in each renderer),
I also compared scenes where I add an arealight to the scene.

<div class="twentytwenty-container">
    <img src="../../validation/spotlight/mitsuba_spotlight.png" alt="Mitsuba spotlight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight.png" alt="Nori spotlight" class="img-responsive">
    <img src="../../validation/spotlight/mitsuba_spotlight+arealight.png" alt="Mitsuba spotlight + arealight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight+arealight.png" alt="Nori spotlight + arealight" class="img-responsive">
</div>

**Area spotlight**

Then I also had the idea of combining these two types into one emitter which works as an arealight except we give the emitter
an inside and outside radiance values as well as maximum and falloff angles (outer and inner). Then the light contribution from a point is the inside radiance
when the angle is less than the falloff then linearly goes to the outside radiance.

**Comparison:**

<div class="twentytwenty-container">
    <img src="../../validation/spotlight/nori_spotlight_area.png" alt="Nori area spotlight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight+arealight.png" alt="Nori spotlight + arealight" class="img-responsive">
    <img src="../../validation/spotlight/nori_arealight.png" alt="Nori arealight" class="img-responsive">
</div>

As we can see this gives smoother shadows.

### Low Discrepancy Sampling
### Mip-Mapping for texture

https://blog.yiningkarlli.com/2018/10/bidirectional-mipmap.html

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_rand_on.png" alt="mipmap" class="img-responsive">
    <img src="../../validation/mipmap/nori_normal_texture_rand_on.png" alt="simple" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_rand_off.png" alt="mipmap" class="img-responsive">
    <img src="../../validation/mipmap/nori_normal_texture_rand_off.png" alt="simple" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_level[3-10].png" alt="mipmap levels" class="img-responsive">
</div>

### Disney BSDF
### Homogeneous Volumetric Participating Media

**Relevant files:** // TODO: update files and description
<ul>
    <li> <code>medium.h</code></li>
    <li> <code>henyey_greenstein.cpp</code></li>
    <li> <code>isotropic.cpp</code></li>
    <li> <code>homogeneous.cpp</code></li>
    <li> <code>shape.h</code></li>
    <li> <code>shape.cpp</code></li>
    <li> <code>mesh.cpp</code></li>
    <li> <code>sphere.cpp</code></li>
    <li> <code>trivial_bsdf.cpp</code></li>
    <li> <code>volumetric_path_mis_simple.cpp</code></li>
    <li> <code>volumetric_path_mis.cpp</code></li>
    <li> <code>camera.h</code></li>
    <li> <code>perspective.cpp</code></li>
</ul>

For one of my advanced features I implemented Volumetric Path Tracing (VPT) in our renderer. It features homogeneous participating media,
various phase functions, MIS and a simple path tracer (MATS)

Medium in the renderer can either be attached to geometric primitives (mesh/sphere) or to the camera. The user can attach an interior and an exterior
medium to shapes and if no medium is attached to a shape then rays bouncing off that shape will stay in the same medium else it will take that shapes medium
(no medium on one side means vacuum). The ray will initially be in the cameras medium.

I also created a trivial bsdf which allows users to create invisible medium boundaries.

**Phase Functions**

Phase functions are a very simple class with only one function, sample, that takes in a vector wi and samples a vector wo. The only reason it is a nori class
in stead of a function is to be able to modularly attach it to media objects. I implemented 2 phase functions, an isotropic one and the Henyey-Greenstein phase function

**Medium**

Medium is a class that has a phase function and in the case of the homogoneous medium absorbing and scattering coefficients for each color channel. On top of that it
has two methods, <code>tr()</code> to calculate the transmission over a given distance and <code>sample()</code> to sample a free path and
a direction at the scattering location.

The homogeneous medium samples by choosing a random color channel, sampling with that channel extinction coefficient and then weighing the results with
multiple importance sampling using the pdfs of all the channels.

**Volumetric Integrators**

I created two integrators, both basen on multiple importance sampling. One that considers media boundaries as occluding when sampling emitters in the scene, this one
matches the volumetric integrator in mitsuba (MIS Simple). The other does not consider media boundaries occluding but calculates the transmittance of the shadow ray and multiplies
it with the emitter sampling contribution (MIS). This gives better performance around boundaries.

**Validation**

Below are comparisions of the simple MIS integrator and the mitsuba integrator first with constant coefficients.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.0_t=1.0.png" alt="MIS &#963<sub>a</sub>=1 &#963<sub>s</sub>=0" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=1.0_t=1.0.png" alt="MIS &#963<sub>a</sub>=0 &#963<sub>s</sub>=1" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.0_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=1 &#963<sub>s</sub>=0" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=1.0_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=0 &#963<sub>s</sub>=1" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.75_t=1.0.png" alt="MIS &#963<sub>a</sub>=0.25 &#963<sub>s</sub>=0.75" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.75_t=20.0.png" alt="MIS &#963<sub>a</sub>=5 &#963<sub>s</sub>=15" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.75_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=0.25 &#963<sub>s</sub>=0.75" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.75_t=20.0.png" alt="Mitsuba &#963<sub>a</sub>=5 &#963<sub>s</sub>=15" class="img-responsive">
</div>

I also compared my two integrators and as we can see that they converge to the same solution.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/nori_box_mis_simple.png" alt="MIS simple spp=512" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_simple_8192.png" alt="MIS simple spp=8192" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis.png" alt="MIS spp=512" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_8192.png" alt="MIS spp=8192" class="img-responsive">
</div>


I also compared scenes with spectrally varying absorption and scattering coefficients, $\sigma_a = [9.0, 0.5, 2.5], \sigma_s = [1.0, 0.5, 2.5]$. Here I compare my
two integrator with the mitsuba renderer as well as using the MIS free path sampling strategy I also render one scene that simply uses the maximum extinction coefficient.
As we can see using MIS for the free path sampling significantly reduces noise when the extinction coefficient has high variety.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/nori_box_mis_color_max_coeff.png" alt="MIS simple max coeff" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.1_t=10.0.png" alt="MIS simple" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.1_t=10.0.png" alt="Mitsuba" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_color.png" alt="MIS" class="img-responsive">

</div>


## Gianluca Figini

### Intel's Open Image Denoise
### Object instancing
The objective of this feature was to allow the existence of multiple instances of the same object without having to each time reload the mesh. To implement this feature, I relied on the structure suggested in an answer to an issue that was given on GitLab page of the project.

To achieve this goal, I created two new classes: `SubScene` and `Instance`. In `Subscene`, a shape and BVH, together with a unique identifier can be stored. On the other hand, `Instance` is a subclass of `Shape` and can, referring to that identifier, physically insert the mesh into the scene. `Instance` also contains a transform describing how the coordinates should be translated from local object coordinates to world coordinates. Since more than one instance can be associated to a subscene, this effectively allows us to create multiple instances of the same object in the scene.

Most functions of `Instance` are inherited or concretized from `Shape`, but some deserve some more explanation. In the first place, the `getPrimitiveCount` function always returns 1. This means that, to the rest of the scene, the instance is a primitive shape, that deals with its BVH internally. Secondly, the `linkSubScene(SubScene *subscene)` is added. In this function, I set the attributes inherited from `Shape`, `m_emitter` and `m_bsdf`. `m_bbox` cannot be set in this way, as the bounding box of the shape stored in subscene is different from the one of the instance. To get from one to the other, the `Instance::getBoundingBox()` function takes the corners of the original bounding box, and creates its own to contain the corresponding points once the `toWorld` transform has been applied.

Some other nori files had to be updated: support for the `Instance` and `Subscene` objects had to be added to `scene.cpp`, `parser.cpp` and `object.h`. It must be mentioned that this implementation causes Nori to crash after finishing rendering any scene using instancing. If time allows, I will look into this in the future.

To validate this approach, I rendered multiple scenes. In the first place, I rendered two spheres from the scene from pa1. This was a simple scene, useful for debugging my code, and testing translation and scaling.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/sphere-normal/two_sphere.png" alt="With Instancing" class="img-responsive">
</div>
I also want to check whether the functions for sampling a surface still worked. To this end, I rendered the `sphere2_ems` scene from pa3, with reduced radiance.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/sphere-emitter/sphere2_ems.png" alt="With Instancing" class="img-responsive">
</div>
Finally, to check whether this feature comes with any benefits in terms of memory and runtime, I wanted to render a more complex scene, so I created a new scene based on the `ajax_av` scene from pa1, both using instancing and simply spawning two meshes.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/ajax/ajax-av-instancing.png" alt="With Instancing" class="img-responsive">
    <img src="../../validation/instancing/ajax/ajax-av-two-meshes.png" alt="Without instancing" class="img-responsive">
</div>
With Instancing        | Primitive count | Memory use | Building time
----------------|-----------------|------------|--------------
BVH Scene       | 4               | 272 B      | 0.0 ms
BVH SubScene    | 544566          | 35.3 MB    | 1.2 s
Total runtime   | -               | 99.3 MB   | 2.3 min

Without Instancing        | Primitive count | Memory use | Building time
----------------|-----------------|------------|--------------
BVH Scene       | 1089134         | 70.6 MB    | 2.8 s
Total runtime   | -               | 176.2 MB  | 1.7 m

Total memory consumption was monitored with Task Manager

### Rendering on the cluster
### Modeling Meshes
I initially had some troubles using the add-on for Blender to export objects to Nori, but with some modifications I got it working. I used Blender 2.90, as Blender 4.0.0 was reported to be too recent for the add-on, and the 2.90 version was mentioned in the GitHub page of the add-on itself. Furthermore, the `Support` argument of the `bl_info` variable had to be changed from `TESTING` to `COMMUNITY`.
**TODO**: Insert description/source of meshes created/used.

### Resampled Importance Sampling (RIS)
In importance sampling, samples are drawn from a probability density function $g$ with values proportional to the function being integrated. However, drawing samples from $g$ requires its integration. RIS provides an alternative approach: it involves obtaining multiple samples $x_i$ from a base distribution $p$, easier to integrate, and selecting one of these with a probability proportional to $g(x_i)/p(x_i)$. To implemet this feature, I've created some new integrators, both for direct illumiation and path tracing.

For direct illumination, our integrator estimates the value of the integral $\int_{H^2} f_r(x, \omega_i, \omega_o) L_i(x, \omega_i) \cos \theta_i \d \omega_i$. A good distribution to use for this estimation is $g(\omega_i) = pdf_{ems}(\omega_i) pdf_{mat}(\omega_i)$. On the other hand, there are multiple possible candidates for $p$: in the file `direct_ris_ems.cpp`, $p = pdf_{ems}$ is used, in `direct_ris_mats.cpp` we use $p = pdf_{mat}$, and in `direct_ris_hemi.cpp` $p$ is cosine distributed on the unit hemisphere.

One can notice that for the EMS integrator, RIS shows the same artifacts present in the importance sampling. This is to be expected, since the sampling function used is the same.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_ems/odyssey_ems.png" alt="Ems importance" class="img-responsive">
    <img src="../../validation/ris/direct_ems/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_ems/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

For the MAT integrator, we can notice that, because of the probability density used, RIS creates some of the artifacts seen for the EMS integrator. However, the noise in the region more distant from the light is significantly reduced.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_mat/odyssey_mats.png" alt="Ems importance" class="img-responsive">
    <img src="../../validation/ris/direct_mat/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_mat/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

Similar results can be observed when sampling the cosine-weighted hemisphere.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_hemi/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_hemi/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

For global illumination, RIS can be used do estimate the BSDF value in a more robust way. In this case, RIS is used to select a direction upon encountering a diffuse surface. This selection is based on the probability density $g = pdf_{mat}$. I've integrated this new selection technique both in the `PathMAT` and in the `PathMIS` integrator.

For the validation, I limited the samples per pixel to 32, to better observe the variance in the images. One can see that, for this application, RIS does not manage to have an effect as significat as in direct illumination. However, especially for the case M=20, a reduction of the noise can still be observed looking at the interior of bowl in the scene.
<div class="twentytwenty-container">
    <img src="../../validation/ris/path_mat/table_path_ris5.png" alt="Mat RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/path_mat/table_path_ris20.png" alt="Mat RIS (M=20)" class="img-responsive">
    <img src="../../validation/ris/path_mat/table_path_mat.png" alt="Mat w/o RIS" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/ris/path_mis/table_path_ris5.png" alt="Mis RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/path_mis/table_path_ris20.png" alt="Mis RIS (M=20)" class="img-responsive">
    <img src="../../validation/ris/path_mis/table_path_mis.png" alt="Mis w/o RIS" class="img-responsive">
</div>

### Many lights sampling

<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.js"></script>

<script>
    $(window).load(function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5 }); });
</script>

<!-- Markdeep: -->
<script>var markdeepOptions = { onLoad: function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5, move_slider_on_hover: true }); }, tocStyle: 'none' };</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep || (document.body.style.visibility = "visible")</script>
