<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

**Report**

Student Names: Árni Bjarnsteinsson and Gianluca Figini

# Implemented features

## Árni Bjarnsteinsson

### Images as Textures

**Relevant files:**
<ul>
    <li> <code>image_texture.cpp</code></li>
</ul>

<div class="twentytwenty-container">
    <img src="../../validation/imagesAsTextures/mitsuba_texture.png" alt="Mitsuba" class="img-responsive">
    <img src="../../validation/imagesAsTextures/nori_texture.png" alt="Nori" class="img-responsive">
</div>

### Simple Emitters (Spotlight)

**Relevant files:**
<ul>
    <li> <code>spotlight.cpp</code></li>
    <li> <code>spotlight_area.cpp</code></li>
</ul>

**Implementation:**

The implementation is very straight forward and is based on the pointlight, except now we also specify a maximum angle of illumnation and another (smaller) angle,
called the falloff angle. The light contribution is exactly like the pointlight if we are within the falloff angle else it is linearly interpolated from 1 to 0
till we reach the maximum angle.


**Validation:**

To validate our spotlight I created a scene in mitsuba and nori (note that the light intensity is slightly different but that is simply implementation detail in each renderer),
I also compared scenes where I add an arealight to the scene.

<div class="twentytwenty-container">
    <img src="../../validation/spotlight/mitsuba_spotlight.png" alt="Mitsuba spotlight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight.png" alt="Nori spotlight" class="img-responsive">
    <img src="../../validation/spotlight/mitsuba_spotlight+arealight.png" alt="Mitsuba spotlight + arealight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight+arealight.png" alt="Nori spotlight + arealight" class="img-responsive">
</div>

**Area spotlight**

Then I also had the idea of combining these two types into one emitter which works as an arealight except we give the emitter
an inside and outside radiance values as well as maximum and falloff angles (outer and inner). Then the light contribution from a point is the inside radiance
when the angle is less than the falloff then linearly goes to the outside radiance.

**Comparison:**

<div class="twentytwenty-container">
    <img src="../../validation/spotlight/nori_spotlight_area.png" alt="Nori area spotlight" class="img-responsive">
    <img src="../../validation/spotlight/nori_spotlight+arealight.png" alt="Nori spotlight + arealight" class="img-responsive">
    <img src="../../validation/spotlight/nori_arealight.png" alt="Nori arealight" class="img-responsive">
</div>

As we can see this gives smoother shadows.

### Low Discrepancy Sampling

<div class="twentytwenty-container">
    <img src="../../validation/halton/independent.png" alt="Independent" class="img-responsive">
    <img src="../../validation/halton/halton.png" alt="Halton" class="img-responsive">
</div>


### Mip-Mapping for texture

https://blog.yiningkarlli.com/2018/10/bidirectional-mipmap.html

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_rand_on.png" alt="mipmap" class="img-responsive">
    <img src="../../validation/mipmap/nori_normal_texture_rand_on.png" alt="simple" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_rand_off.png" alt="mipmap" class="img-responsive">
    <img src="../../validation/mipmap/nori_normal_texture_rand_off.png" alt="simple" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/mipmap/nori_mipmap_level[3-10].png" alt="mipmap levels" class="img-responsive">
</div>

### Disney BSDF

eval function: https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf#L119

sampling weights: https://cgl.ethz.ch/teaching/cg23/www-nori/example_reports/mylonopoulos_sancho/report.html

**Roughness**
<div class="twentytwenty-container">
    <img src="../../validation/disney/nori_roughness.png" alt="Nori" class="img-responsive">
    <img src="../../validation/disney/mitsuba_disney_roughness.png" alt="Mitsuba" class="img-responsive">
</div>

**Specular**
<div class="twentytwenty-container">
    <img src="../../validation/disney/nori_specular.png" alt="Nori" class="img-responsive">
    <img src="../../validation/disney/mitsuba_disney_specular.png" alt="Mitsuba" class="img-responsive">
</div>

**Metallic**
<div class="twentytwenty-container">
    <img src="../../validation/disney/nori_metallic.png" alt="Nori" class="img-responsive">
    <img src="../../validation/disney/mitsuba_disney_metallic.png" alt="Mitsuba" class="img-responsive">
</div>

**Anisotropic**
<div class="twentytwenty-container">
    <img src="../../validation/disney/nori_anisotropic.png" alt="Nori" class="img-responsive">
    <img src="../../validation/disney/mitsuba_disney_anisotropic.png" alt="Mitsuba" class="img-responsive">
</div>

**Subsurface**
<div class="twentytwenty-container">
    <img src="../../validation/disney/nori_subsurface.png" alt="Nori" class="img-responsive">
    <img src="../../validation/disney/mitsuba_disney_flatness.png" alt="Mitsuba" class="img-responsive">
</div>



### Homogeneous Volumetric Participating Media

**Relevant files:** // TODO: update files and description
<ul>
    <li> <code>medium.h</code></li>
    <li> <code>henyey_greenstein.cpp</code></li>
    <li> <code>isotropic.cpp</code></li>
    <li> <code>homogeneous.cpp</code></li>
    <li> <code>shape.h</code></li>
    <li> <code>shape.cpp</code></li>
    <li> <code>mesh.cpp</code></li>
    <li> <code>sphere.cpp</code></li>
    <li> <code>trivial_bsdf.cpp</code></li>
    <li> <code>volumetric_path_mis_simple.cpp</code></li>
    <li> <code>volumetric_path_mis.cpp</code></li>
    <li> <code>camera.h</code></li>
    <li> <code>perspective.cpp</code></li>
</ul>

For one of my advanced features I implemented Volumetric Path Tracing (VPT) in our renderer. It features homogeneous participating media,
various phase functions, MIS and a simple path tracer (MATS)

Medium in the renderer can either be attached to geometric primitives (mesh/sphere) or to the camera. The user can attach an interior and an exterior
medium to shapes and if no medium is attached to a shape then rays bouncing off that shape will stay in the same medium else it will take that shapes medium
(no medium on one side means vacuum). The ray will initially be in the cameras medium.

I also created a trivial bsdf which allows users to create invisible medium boundaries.

**Phase Functions**

Phase functions are a very simple class with only one function, sample, that takes in a vector wi and samples a vector wo. The only reason it is a nori class
in stead of a function is to be able to modularly attach it to media objects. I implemented 2 phase functions, an isotropic one and the Henyey-Greenstein phase function

**Medium**

Medium is a class that has a phase function and in the case of the homogoneous medium absorbing and scattering coefficients for each color channel. On top of that it
has two methods, <code>tr()</code> to calculate the transmission over a given distance and <code>sample()</code> to sample a free path and
a direction at the scattering location.

The homogeneous medium samples by choosing a random color channel, sampling with that channel extinction coefficient and then weighing the results with
multiple importance sampling using the pdfs of all the channels.

**Volumetric Integrators**

I created two integrators, both basen on multiple importance sampling. One that considers media boundaries as occluding when sampling emitters in the scene, this one
matches the volumetric integrator in mitsuba (MIS Simple). The other does not consider media boundaries occluding but calculates the transmittance of the shadow ray and multiplies
it with the emitter sampling contribution (MIS). This gives better performance around boundaries.

**Validation**

Below are comparisions of the simple MIS integrator and the mitsuba integrator first with constant coefficients.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.0_t=1.0.png" alt="MIS &#963<sub>a</sub>=1 &#963<sub>s</sub>=0" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=1.0_t=1.0.png" alt="MIS &#963<sub>a</sub>=0 &#963<sub>s</sub>=1" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.0_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=1 &#963<sub>s</sub>=0" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=1.0_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=0 &#963<sub>s</sub>=1" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.75_t=1.0.png" alt="MIS &#963<sub>a</sub>=0.25 &#963<sub>s</sub>=0.75" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.75_t=20.0.png" alt="MIS &#963<sub>a</sub>=5 &#963<sub>s</sub>=15" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.75_t=1.0.png" alt="Mitsuba &#963<sub>a</sub>=0.25 &#963<sub>s</sub>=0.75" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.75_t=20.0.png" alt="Mitsuba &#963<sub>a</sub>=5 &#963<sub>s</sub>=15" class="img-responsive">
</div>

I also compared my two integrators and as we can see that they converge to the same solution.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/nori_box_mis_simple.png" alt="MIS simple spp=512" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_simple_8192.png" alt="MIS simple spp=8192" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis.png" alt="MIS spp=512" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_8192.png" alt="MIS spp=8192" class="img-responsive">
</div>


I also compared scenes with spectrally varying absorption and scattering coefficients, $\sigma_a = [9.0, 0.5, 2.5], \sigma_s = [1.0, 0.5, 2.5]$. Here I compare my
two integrator with the mitsuba renderer as well as using the MIS free path sampling strategy I also render one scene that simply uses the maximum extinction coefficient.
As we can see using MIS for the free path sampling significantly reduces noise when the extinction coefficient has high variety.

<div class="twentytwenty-container">
    <img src="../../validation/homogeneous/nori_box_mis_color_max_coeff.png" alt="MIS simple max coeff" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mis_albedo=0.1_t=10.0.png" alt="MIS simple" class="img-responsive">
    <img src="../../validation/homogeneous/cbox_mitsuba_albedo=0.1_t=10.0.png" alt="Mitsuba" class="img-responsive">
    <img src="../../validation/homogeneous/nori_box_mis_color.png" alt="MIS" class="img-responsive">

</div>


## Gianluca Figini

### Intel's Open Image Denoise
### Object instancing
**Relevant files**
<ul>
    <li> <code>subscene.h</code></li>
    <li> <code>subscene.cpp</code></li>
    <li> <code>instance.h</code></li>
    <li> <code>instance.cpp</code></li>
    <li> <code>scene.h</code></li>
    <li> <code>scene.cpp</code></li>
    <li> <code>object.h</code></li>
    <li> <code>parser.cpp</code></li>
</ul>
The objective of this feature was to allow the existence of multiple instances of the same object without having to each time reload the mesh. To implement this feature, I relied on the structure suggested in an answer to an issue that was given on GitLab page of the project.

To achieve this goal, I created two new classes: `SubScene` and `Instance`. In `Subscene`, a shape and BVH, together with a unique identifier can be stored. On the other hand, `Instance` is a subclass of `Shape` and can, referring to that identifier, physically insert the mesh into the scene. `Instance` also contains a transform describing how the coordinates should be translated from local object coordinates to world coordinates. Since more than one instance can be associated to a subscene, this effectively allows us to create multiple instances of the same object in the scene.

Most functions of `Instance` are inherited or concretized from `Shape`, but some deserve some more explanation. In the first place, the `getPrimitiveCount` function always returns 1. This means that, to the rest of the scene, the instance is a primitive shape, that deals with its BVH internally. Secondly, the `linkSubScene(SubScene *subscene)` is added. In this function, I set the attributes inherited from `Shape`, `m_emitter` and `m_bsdf`. `m_bbox` cannot be set in this way, as the bounding box of the shape stored in subscene is different from the one of the instance. To get from one to the other, the `Instance::getBoundingBox()` function takes the corners of the original bounding box, and creates its own to contain the corresponding points once the `toWorld` transform has been applied.

Some other nori files had to be updated: support for the `Instance` and `Subscene` objects had to be added to `scene.cpp`, `parser.cpp` and `object.h`. It must be mentioned that this implementation causes Nori to crash after finishing rendering any scene using instancing. If time allows, I will look into this in the future.

To validate this approach, I rendered multiple scenes. In the first place, I rendered two spheres from the scene from pa1. This was a simple scene, useful for debugging my code, and testing translation and scaling.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/sphere-normal/two_sphere.png" alt="With Instancing" class="img-responsive">
</div>
I also want to check whether the functions for sampling a surface still worked. To this end, I rendered the `sphere2_ems` scene from pa3, with reduced radiance.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/sphere-emitter/sphere2_ems.png" alt="With Instancing" class="img-responsive">
</div>
Finally, to check whether this feature comes with any benefits in terms of memory and runtime, I wanted to render a more complex scene, so I created a new scene based on the `ajax_av` scene from pa1, both using instancing and simply spawning two meshes.
<div class="twentytwenty-container">
    <img src="../../validation/instancing/ajax/ajax-av-instancing.png" alt="With Instancing" class="img-responsive">
    <img src="../../validation/instancing/ajax/ajax-av-two-meshes.png" alt="Without instancing" class="img-responsive">
</div>
With Instancing        | Primitive count | Memory use | Building time
----------------|-----------------|------------|--------------
BVH Scene       | 4               | 272 B      | 0.0 ms
BVH SubScene    | 544566          | 35.3 MB    | 1.2 s
Total runtime   | -               | 99.3 MB   | 2.3 min

Without Instancing        | Primitive count | Memory use | Building time
----------------|-----------------|------------|--------------
BVH Scene       | 1089134         | 70.6 MB    | 2.8 s
Total runtime   | -               | 176.2 MB  | 1.7 m

Total memory consumption was monitored with Task Manager

### Rendering on the cluster
### Modeling Meshes
I initially had some troubles using the add-on for Blender to export objects to Nori, but with some modifications I got it working. I used Blender 2.90, as Blender 4.0.0 was reported to be too recent for the add-on, and the 2.90 version was mentioned in the GitHub page of the add-on itself. Furthermore, the `Support` argument of the `bl_info` variable had to be changed from `TESTING` to `COMMUNITY`.
TODO: Insert description/source of meshes created/used.

### Resampled Importance Sampling (RIS)
**Relevant files**
<ul>
    <li> <code>direct_ris_ems.cpp</code></li>
    <li> <code>direct_ris_mats.cpp</code></li>
    <li> <code>direct_ris_mis.cpp</code></li>
    <li> <code>path_ris_mat.cpp</code></li>
    <li> <code>path_ris_mis.cpp</code></li>
</ul>
In importance sampling, samples are drawn from a probability density function $g$ with values proportional to the function being integrated. However, drawing samples from $g$ requires its integration. RIS provides an alternative approach: it involves obtaining multiple samples $x_i$ from a base distribution $p$, easier to integrate, and selecting one of these with a probability proportional to $g(x_i)/p(x_i)$. To implemet this feature, I've created some new integrators, both for direct illumiation and path tracing.

For direct illumination, our integrator estimates the value of the integral $\int_{H^2} f_r(x, \omega_i, \omega_o) L_i(x, \omega_i) \cos \theta_i \d \omega_i$. A good distribution to use for this estimation is $g(\omega_i) = pdf_{ems}(\omega_i) pdf_{mat}(\omega_i)$. On the other hand, there are multiple possible candidates for $p$: in the file `direct_ris_ems.cpp`, $p = pdf_{ems}$ is used, in `direct_ris_mats.cpp` we use $p = pdf_{mat}$, and in `direct_ris_hemi.cpp` $p$ is cosine distributed on the unit hemisphere.

One can notice that for the EMS integrator, RIS shows the same artifacts present in the importance sampling. This is to be expected, since the sampling function used is the same.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_ems/odyssey_ems.png" alt="Ems importance" class="img-responsive">
    <img src="../../validation/ris/direct_ems/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_ems/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

For the MAT integrator, we can notice that, because of the probability density used, RIS creates some of the artifacts seen for the EMS integrator. However, the noise in the region more distant from the light is significantly reduced.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_mat/odyssey_mats.png" alt="Ems importance" class="img-responsive">
    <img src="../../validation/ris/direct_mat/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_mat/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

Similar results can be observed when sampling the cosine-weighted hemisphere.
<div class="twentytwenty-container">
    <img src="../../validation/ris/direct_hemi/odyssey_ris5.png" alt="Ems RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/direct_hemi/odyssey_ris20.png" alt="Ems RIS (M=20)" class="img-responsive">
</div>

For global illumination, RIS can be used do estimate the BSDF value in a more robust way. In this case, RIS is used to select a direction upon encountering a diffuse surface. This selection is based on the probability density $g = pdf_{mat}$. I've integrated this new selection technique both in the `PathMAT` and in the `PathMIS` integrator.

For the validation, I limited the samples per pixel to 32, to better observe the variance in the images. One can see that, for this application, RIS does not manage to have an effect as significat as in direct illumination. However, especially for the case M=20, a reduction of the noise can still be observed looking at the interior of bowl in the scene.
<div class="twentytwenty-container">
    <img src="../../validation/ris/path_mat/table_path_ris5.png" alt="Mat RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/path_mat/table_path_ris20.png" alt="Mat RIS (M=20)" class="img-responsive">
    <img src="../../validation/ris/path_mat/table_path_mat.png" alt="Mat w/o RIS" class="img-responsive">
</div>

<div class="twentytwenty-container">
    <img src="../../validation/ris/path_mis/table_path_ris5.png" alt="Mis RIS (M=5)" class="img-responsive">
    <img src="../../validation/ris/path_mis/table_path_ris20.png" alt="Mis RIS (M=20)" class="img-responsive">
    <img src="../../validation/ris/path_mis/table_path_mis.png" alt="Mis w/o RIS" class="img-responsive">
</div>

### Many lights sampling
**Relevant files**
<ul>
    <li> <code>emitter.h</code></li>
    <li> <code>instance.h</code></li>
    <li> <code>lightbvh.h</code></li>
    <li> <code>lightcone.h</code></li>
    <li> <code>mesh.h</code></li>
    <li> <code>scene.h</code></li>
    <li> <code>shape.h</code></li>
    <li> <code>subscene.h</code></li>
    <li> <code>arealight.cpp</code></li>
    <li> <code>direct_ems.cpp</code></li>
    <li> <code>direct_mis.cpp</code></li>
    <li> <code>lightbvh.cpp</code></li>
    <li> <code>mesh.cpp</code></li>
    <li> <code>path_mis.cpp</code></li>
    <li> <code>pointlight.cpp</code></li>
    <li> <code>scene.cpp</code></li>
    <li> <code>sphere.cpp</code></li>
    <li> <code>spotlight.cpp</code></li>
    <li> <code>spotlight_area.cpp</code></li>
</ul>
For this feature, I implemented the approach to the many light rendering problem described in "Estevez A.C. and Kulla C., Importance Sampling of Many Lights with Adaptive Tree Splitting, 2018". In syntesis, this approach consists of handling the emitter in a scene using a bounding volume hierarcy, which allows the renderer to determine the relative importance of an emitter when shading a specific point in the scene.

For keeping track of the additional data that the technique described in the paper uses, I created a new data structure: the Light Cone. This datastructure is linked to a `LightBVHNode` (a cluster of lights) and keeps track of an axis and two different angles, `theta_o` that describes a cone around the axis including all the normals to the emitters in the cluster, and an additional angle `theta_o` which also accounts for the falloff of the meitters. In the file `lightcone.h`, this struct is implemented togheter with some utility functions, in particular to merge different cones togheter.

In order to obtain these information from each Emitter, I added the functions `getLightCone()` and `getPower()` to the `emitter.h` interface. Since the `getLightCone()` function can depend on the shape of the emitter, I also added it to every sahpe and to the `shape.h` interface.

For implemeting this hierarcy, I started taking the already present `bvh.{h, cpp}` files as reference, and changing it as needed in `lightbvh.{h, cpp}`. The first thing I needed to alter was the content of a node: the struct `LightBVHNode` used in this BVH contains, in addition to the bounding box of the light cluster, also the associated Light Cone and the power measure. Consequently, I also needed to change the `struct Bins` and the building procedure of the hierarcy to account for these additional measures. In particular, the Surface Area Heuristic (SAH) was changed with the Surface Area Orientation Heuristic (SAOH) described in the paper.  Since I did not completely comprehend the stopping criteria described in the paper to define when to create a leaf node, I amplified the initial `bestCost` variable in the original code to incentivize the process to separate each emitter into its own node. Changing the code for the building process was particularly challenging, both because of the sheer complexity of the method used (I had to refer back to the article describing this technique), and because the `execute_sequentially` function was not designed to store the bounding box into the child node (and clearly not the light cone and power value either).

The tree traversal is implemented in the `sample` function of `LightBVH`. As described in the paper, this happend stochastically, selecting each time one of the two chidlren of the current node based on their importance with respect to the shading point. The importance function, detailed in the paper, depends on the distance between the shading point and the center of the cluster of lights and the power of the lights themselves, as well as on the cosine of the angle formed by the vector connecting these two points and  the axis of the cone associated with the cluster. To sample this tree, we use a new query record datastructure, the `LightBVHQueryRecord`, which contains the coordinates of the shading point and the relative surface normal, and is used to return the pdf of the sampling. To inisiate this whole operation, I created an alternative `getRandomEmitter` function in the `scene.h` interface, which directly calls `sample`.

Finally, I implemented a `pdf` function that, given a `LightBVHQueryRecord` and a reference to the selected emitter, traverses the tree and returns the probability of sampling tthe given emitter while shading the current point. This function can get called from the integrator through the `getRandomEmitterPdf` function in the scene interface.

To verificate this approach, I first of all verified that this new emitter sampling method did not cause any bias. Then, I rendered the `veach` scene from pa3, using both the `direct_ems` and the `direct_mis` integrators (`direct_mats` does not call the `getRandomEmitter` function). We can notice under the largest sphere that using this technique, more samples are correctly directed towards the bigger emitter, resulting in a larger number of points on the specular surfaces beneath.

<div class="twentytwenty-container">
    <img src="../../validation/manylights/veach/ems_ref.png" alt="EMS" class="img-responsive">
    <img src="../../validation/manylights/veach/veach_ems.png" alt="EMS with manylights" class="img-responsive">
    <img src="../../validation/manylights/veach/mis_ref.png" alt="MIS" class="img-responsive">
    <img src="../../validation/manylights/veach/veach_mis.png" alt="MIS with manylights" class="img-responsive">
</div>

TODO scene with more emitters

<!-- Bootstrap core CSS and JavaScript -->

<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="../resources/bootstrap.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.js"></script>

<script>
    $(window).load(function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5 }); });
</script>

<!-- Markdeep: -->
<script>var markdeepOptions = { onLoad: function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5, move_slider_on_hover: true }); }, tocStyle: 'none' };</script>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep || (document.body.style.visibility = "visible")</script>
